{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "\n",
    "This will work if this notebook is placed in this folder (EzproxyProj/data/ezproxylogs/) \n",
    "\n",
    "\n",
    "Following folder structure is needed : EzproxyProj/data/ezproxylogs/csv/offcampus/201712 \n",
    "\n",
    "Add one month's worth of daily log files to the same directory that has this file in it. In the same directory, create a folder called 'csv' with matching subfolder (offcampus/201712) \n",
    "## Step 2: \n",
    "Run the code in the next box - click in box and press Shift+Enter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count domain hits\n",
    "\n",
    "# Code by Petrina Collingwood \n",
    "# Modified by Josephine Choi \n",
    "\n",
    "def domain_count(filename):\n",
    "    \n",
    "    import csv\n",
    "    import re\n",
    "    # create csv file from log file\n",
    "    with open(filename,'r') as fh:\n",
    "        with open('csv/' + filename + '.csv','w') as outfile:\n",
    "            for line in fh:\n",
    "                print(re.sub(r'\\n|\"','',line), file=outfile)\n",
    "    import pandas as pd\n",
    "    from urllib.parse import unquote\n",
    "    # create dataframe from csv file skipping malformed lines\n",
    "    df = pd.read_csv('csv/' + filename + '.csv',sep=' ', error_bad_lines=False, header=None, encoding='utf-8')\n",
    "    # remove unnecessary columns\n",
    "    df.drop(df.columns[[2,5,6,8,9]], axis=1, inplace=True)\n",
    "    # name columns\n",
    "    df.columns = ['ip', 'session_id', 'user_id', 'date_time', 'url', 'size']\n",
    "    # formate date/time column\n",
    "    df['date_time'] = df['date_time'].map(lambda x: x.lstrip('['))\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], format='%d/%b/%Y:%H:%M:%S')\n",
    "    # remove lines where user is not logged in\n",
    "    df = df[df.user_id != \"-\"]\n",
    "    # decode urls\n",
    "    def decode_url(url):\n",
    "        decoded_url = unquote(url)\n",
    "        return decoded_url\n",
    "    df['url'] = df.url.apply(decode_url)\n",
    "    # remove excess columns for domain\n",
    "    df.drop(['ip','session_id','size'], axis=1, inplace=True)\n",
    "    # remove ezp string from start of url\n",
    "    df['url'] = df['url'].str.replace(r'^http://ezproxy\\.lib\\.ryerson\\.ca/login/\\?url=', '')# remove http etc\n",
    "    df['url'] = df['url'].str.replace(r'^http://www\\.|^https://www\\.|^http://|^https://', '')\n",
    "    # remove ezproxy string from start of url\n",
    "    def parse_url(url):\n",
    "        if (url.startswith(\"ezproxy.lib.ryerson.ca/login?url=\")) and (\"http\" in url):\n",
    "            location = url.find(\"http\")\n",
    "            return url[location:]\n",
    "        elif (url.startswith(\"ezproxy.lib.ryerson.ca/login?url=\")):\n",
    "            return \"-\"\n",
    "        else:\n",
    "            return url\n",
    "    df['url'] = df.url.apply(parse_url)\n",
    "    # remove http etc\n",
    "    df['url'] = df['url'].str.replace(r'^http://www\\.|^https://www\\.|^http://|^https://', '')\n",
    "    # remove rows where ezproxy string is the only url\n",
    "    df = df[df.url != \"-\"]\n",
    "    # remove spaces introduced by unquoting\n",
    "    df['url'] = df['url'].str.replace(r'\\n', '')\n",
    "    # remove everything after : or / or ?\n",
    "    df['url'] = df['url'].str.replace(r'[:/?].*$', '')\n",
    "    # remove .ezp.lib.unimelb.edu.au from urls\n",
    "    df['url'] = df['url'].str.replace(r'ezproxy\\.lib\\.ryerson\\.ca', '')\n",
    "    df['url'] = df['url'].str.replace(r'ezproxy\\.lib\\.ryerson\\.ca', '-')\n",
    "    df = df[df.url != \"-\"]\n",
    "    # create new column of domains\n",
    "    def get_domain(url):\n",
    "        regexp = re.compile(r'\\.com|\\.org|\\.net|\\.edu|-org|-com|\\.gov')\n",
    "        if regexp.search(url) is not None:\n",
    "            for match in regexp.finditer(url):\n",
    "                location = match.start()\n",
    "            new_url = url[:location]\n",
    "            if ('.' in new_url):\n",
    "                location = new_url.rfind('.')\n",
    "            elif ('-' in new_url):\n",
    "                location = new_url.rfind('-')\n",
    "            else:\n",
    "                return url\n",
    "            location += 1\n",
    "            \n",
    "            if (\"-org\" in url[location:]):\n",
    "                modified_url = url[location:].replace(r'-org', '.org')\n",
    "            elif (\"-com\" in url[location:]):\n",
    "                modified_url = url[location:].replace(r'-com', '.com')\n",
    "            else:\n",
    "                return url[location:]\n",
    "            return modified_url\n",
    "        else:\n",
    "            return url\n",
    "    df['domain'] = df.url.apply(get_domain)\n",
    "    # remove duplicate rows which have same user_id, date-time and domain. \n",
    "    df.drop_duplicates(subset=['user_id', 'date_time','domain'], inplace=True)\n",
    "    df_domains = df['domain'].value_counts().reset_index()\n",
    "    # rename columns\n",
    "    df_domains.columns = ['domain', 'count']\n",
    "    # create csv file from daily domain count dataframe\n",
    "    df_domains.to_csv('daily_domains.csv',index=False, encoding='utf-8')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: \n",
    "Run the code in the following box. This will run for up to an hour. \n",
    "\n",
    "Wait until the circle in the top right corner is no longer full before moving on to next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# main code to process analyse daily log files and create domain count csv file\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "# loop through files in current directory which begin with 'ezproxy.log.*'\n",
    "# modify this if your log file name starts with something else\n",
    "# JC: I have modified this so that it will look for the log files in a particular folder \"/off/2012\"\n",
    "# JC: This also means that I do not have to delete the csv files later. However, I believe I do need to create separate folders under the \"csv\" folder \n",
    "\n",
    "\n",
    "for filename in glob.iglob(os.path.join(\"offcampus/201806/\",\"*.log\")):\n",
    "    filename = filename.replace(\"\\\\\",\"/\")\n",
    "    # call log analysis function \n",
    "    domain_count(filename)\n",
    "    # create dataframe from csv output file from daily domain count\n",
    "    df_daily = pd.read_csv('daily_domains.csv',sep=',', encoding='utf-8')\n",
    "    import os.path\n",
    "    # if domains.csv already exists - add daily domain counts to domains.csv\n",
    "    if os.path.isfile('domains.csv'):\n",
    "        # create dataframe from domains.csv\n",
    "        df = pd.read_csv('domains.csv',sep=',')\n",
    "        # rename daily dataframe count column to 'daily_count'\n",
    "        df_daily.rename(columns={'count': 'daily_count'}, inplace=True)\n",
    "        # merge the daily and accumulative dataframes\n",
    "        df_merge = pd.merge(df, df_daily, on='domain', how='outer')\n",
    "        # replace all NaN values with 0\n",
    "        df_merge.fillna(0, inplace=True)\n",
    "        # add new column with total of the two count columns\n",
    "        df_merge['total_count'] = df_merge['count'] + df_merge['daily_count']\n",
    "        # drop first 2 columns\n",
    "        df_merge.drop(df_merge.columns[[1,2]], axis=1, inplace=True)\n",
    "        # rename count column\n",
    "        df_merge.rename(columns={'total_count': 'count'}, inplace=True)\n",
    "        # create csv file from resulting dataframe\n",
    "        df_merge.to_csv('domains.csv',index=False, encoding='utf-8')\n",
    "    # first time through, convert domain count dataframe results to csv file\n",
    "    else:\n",
    "        df_daily.to_csv('domains.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4\n",
    "\n",
    "Check that the domains.csv file looks ok. It should have 2 columns: domain and count.\n",
    "\n",
    "Rename it to something like this: '2015_07_ezproxy_database_usage.csv'\n",
    "Delete daily_domains.csv\n",
    "Delete all the csv files in the csv directory.\n",
    "Delete all of the log files that you have just analysed and copy and paste in the next month's worth of log\n",
    "files to be analysed.\n",
    "\n",
    "## Step 5:\n",
    "Repeat steps 3 and 4 until you have analysed all of the months that you want to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6:\n",
    "\n",
    "For each monthly file you created in the above process, you need to run the code in the next box. \n",
    "Each time you run it, you will need to modify the filename to the file that you are running.\n",
    "So this code starts with June 2016 - '2016_06_ezproxy_database_usage.csv'. The first time you run the code,\n",
    "you also need to change the column name in the 'else' section to the month name, in this case it's 'Jun-16'. \n",
    "For subsequent times you run it, change the month column name in the 'if' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines monthly files into one spreadsheet\n",
    "import csv\n",
    "import pandas as pd\n",
    "# create dataframe from monthly domain count csv file\n",
    "# each time you run this code, change the file name as you work through each month\n",
    "# creates dataframe from monthly totals csv file\n",
    "df_one = pd.read_csv('2018_06_ezproxy_database_usage_offcampus.csv',sep=',', encoding='utf-8')\n",
    "import os.path\n",
    "# second or more run through\n",
    "# merge monthly file with totals file\n",
    "if os.path.isfile('2017-2018_ezproxy_usage_monthly_totals_offcampus.csv'):\n",
    "    # create dataframe from existing totals csv file\n",
    "    df_two = pd.read_csv('2017-2018_ezproxy_usage_monthly_totals_offcampus.csv',sep=',', encoding='utf-8')\n",
    "    #drop rows with domain = 0\n",
    "    df_two = df_two[df_two.domain != \"0\"]\n",
    "    # merge two dataframes\n",
    "    df_merge = pd.merge(df_one, df_two, on='domain', how='outer')\n",
    "    # remove NaN values with 0\n",
    "    df_merge.fillna(0, inplace=True)\n",
    "    # rename count column to month/year\n",
    "    # change 'Jul-15' to whatever month file you are running\n",
    "    df_merge.rename(columns={'count': 'Jun-18'}, inplace=True)\n",
    "    df_merge = df_merge[df_merge.domain != \"0\"]\n",
    "    # create csv from resulting dataframe\n",
    "    df_merge.to_csv('2017-2018_ezproxy_usage_monthly_totals_offcampus.csv',index=False, encoding='utf-8')\n",
    "# first run through\n",
    "else:\n",
    "    # rename count of first monthly count column, so replace 'Jun-16' with the month you are starting with\n",
    "    df_one.rename(columns={'count': 'Jun-18'}, inplace=True)\n",
    "    #drop rows with domain = 0\n",
    "    df_one = df_one[df_one.domain != \"0\"]\n",
    "    # convert dataframe to csv\n",
    "    df_one.to_csv('2017-2018_ezproxy_usage_monthly_totals_offcampus.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: \n",
    "\n",
    "Depending on how many months and which months you are processing, rename columns in the code below.\n",
    "Run the code below and your csv spreadsheet is complete. You can then open it in Excel, make it prettier and\n",
    "save it as a .xlsx file if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Sep-16'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2441\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2442\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2443\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Sep-16'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-25b147b4af26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2016-2017_ezproxy_usage_monthly_totals.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Modified (from biannual total to total per semester)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sep-Dec_16_Total'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sep-16'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Oct-16'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Nov-16'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Dec-16'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Jan-Apr_17_Total'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Jan-17'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Feb-17'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Mar-17'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Apr-17'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'May-Aug_17_Total'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'May-17'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Jun-17'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Jul-17'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Aug-17'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1962\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1964\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1966\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1969\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1970\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1971\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1645\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3590\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3591\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2442\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2443\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2444\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Sep-16'"
     ]
    }
   ],
   "source": [
    "# create 6-month and yearly total columns\n",
    "import csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('2016-2017_ezproxy_usage_monthly_totals.csv',sep=',', encoding='utf-8')\n",
    "# Modified (from biannual total to total per semester)\n",
    "df['Sep-Dec_16_Total'] = df['Sep-16'] + df['Oct-16'] + df['Nov-16'] + df['Dec-16']\n",
    "df['Jan-Apr_17_Total'] = df['Jan-17'] + df['Feb-17'] + df['Mar-17'] + df['Apr-17'] \n",
    "df['May-Aug_17_Total'] = df['May-17'] + df['Jun-17'] + df['Jul-17'] + df['Aug-17']\n",
    "#df['Yearly_Total'] = df['Jul-Dec_13_Total'] + df['Jan-Jun_16_Total']\n",
    "df['Yearly_Total'] = df['Sep-Dec_16_Total'] + df['Jan-Apr_17_Total'] + df['May-Aug_17_Total']\n",
    "#sort on Yearly_Total\n",
    "df.sort_values(by='Yearly_Total', ascending=0, inplace=True)\n",
    "# create csv file from dataframe\n",
    "df.to_csv('2016-2017_ezproxy_usage_monthly_totals.csv',index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
