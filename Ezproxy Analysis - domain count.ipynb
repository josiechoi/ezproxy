{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "In the same directory, create a folder called 'csv'. \n",
    "\n",
    "Note: With this particular script, step one does not require the user to manually add the log files to the directory. \n",
    "However, you do need to modified the line in Step 3 \"for filename in glob.iglob(os.path.join(\"offcampus/201712/\",\"*.log\")):\" \n",
    "so that it will find the correct folder.\n",
    "\n",
    "Also, you will need to create a subfolder under the folder \"csv\" for the month (e.g. offcampus/201712) \n",
    "\n",
    "## Step 2: \n",
    "Run the code in the next box - click in box and press Shift+Enter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to count domain hits\n",
    "\n",
    "# Code by Petrina Collingwood \n",
    "# Modified by Josephine Choi \n",
    "\n",
    "def domain_count(filename):\n",
    "    \n",
    "    import csv\n",
    "    import re\n",
    "    # create csv file from log file\n",
    "    with open(filename,'r') as fh:\n",
    "        with open('csv/' + filename + '.csv','w') as outfile:\n",
    "            for line in fh:\n",
    "                print(re.sub(r'\\n|\"','',line), file=outfile)\n",
    "    import pandas as pd\n",
    "    from urllib.parse import unquote\n",
    "    # create dataframe from csv file skipping malformed lines\n",
    "    df = pd.read_csv('csv/' + filename + '.csv',sep=' ', error_bad_lines=False, header=None, encoding='utf-8')\n",
    "    # remove unnecessary columns\n",
    "    df.drop(df.columns[[2,5,6,8,9]], axis=1, inplace=True)\n",
    "    # name columns\n",
    "    df.columns = ['ip', 'session_id', 'user_id', 'date_time', 'url', 'size']\n",
    "    # formate date/time column\n",
    "    df['date_time'] = df['date_time'].map(lambda x: x.lstrip('['))\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], format='%d/%b/%Y:%H:%M:%S')\n",
    "    # remove lines where user is not logged in\n",
    "    df = df[df.user_id != \"-\"]\n",
    "    # decode urls\n",
    "    def decode_url(url):\n",
    "        decoded_url = unquote(url)\n",
    "        return decoded_url\n",
    "    df['url'] = df.url.apply(decode_url)\n",
    "    # remove excess columns for domain\n",
    "    df.drop(['ip','session_id','size'], axis=1, inplace=True)\n",
    "    # remove ezp string from start of url\n",
    "    df['url'] = df['url'].str.replace(r'^http://ezproxy\\.lib\\.ryerson\\.ca/login/\\?url=', '')# remove http etc\n",
    "    df['url'] = df['url'].str.replace(r'^http://www\\.|^https://www\\.|^http://|^https://', '')\n",
    "    # remove ezproxy string from start of url\n",
    "    def parse_url(url):\n",
    "        if (url.startswith(\"ezproxy.lib.ryerson.ca/login?url=\")) and (\"http\" in url):\n",
    "            location = url.find(\"http\")\n",
    "            return url[location:]\n",
    "        elif (url.startswith(\"ezproxy.lib.ryerson.ca/login?url=\")):\n",
    "            return \"-\"\n",
    "        else:\n",
    "            return url\n",
    "    df['url'] = df.url.apply(parse_url)\n",
    "    # remove http etc\n",
    "    df['url'] = df['url'].str.replace(r'^http://www\\.|^https://www\\.|^http://|^https://', '')\n",
    "    # remove rows where ezproxy string is the only url\n",
    "    df = df[df.url != \"-\"]\n",
    "    # remove spaces introduced by unquoting\n",
    "    df['url'] = df['url'].str.replace(r'\\n', '')\n",
    "    # remove everything after : or / or ?\n",
    "    df['url'] = df['url'].str.replace(r'[:/?].*$', '')\n",
    "    # remove .ezp.lib.unimelb.edu.au from urls\n",
    "    df['url'] = df['url'].str.replace(r'ezproxy\\.lib\\.ryerson\\.ca', '')\n",
    "    df['url'] = df['url'].str.replace(r'ezproxy\\.lib\\.ryerson\\.ca', '-')\n",
    "    df = df[df.url != \"-\"]\n",
    "    # create new column of domains\n",
    "    def get_domain(url):\n",
    "        regexp = re.compile(r'\\.com|\\.org|\\.net|\\.edu|-org|-com|\\.gov')\n",
    "        if regexp.search(url) is not None:\n",
    "            for match in regexp.finditer(url):\n",
    "                location = match.start()\n",
    "            new_url = url[:location]\n",
    "            if ('.' in new_url):\n",
    "                location = new_url.rfind('.')\n",
    "            elif ('-' in new_url):\n",
    "                location = new_url.rfind('-')\n",
    "            else:\n",
    "                return url\n",
    "            location += 1\n",
    "            \n",
    "            if (\"-org\" in url[location:]):\n",
    "                modified_url = url[location:].replace(r'-org', '.org')\n",
    "            elif (\"-com\" in url[location:]):\n",
    "                modified_url = url[location:].replace(r'-com', '.com')\n",
    "            else:\n",
    "                return url[location:]\n",
    "            return modified_url\n",
    "        else:\n",
    "            return url\n",
    "    df['domain'] = df.url.apply(get_domain)\n",
    "    # remove duplicate rows which have same user_id, date-time and domain. \n",
    "    df.drop_duplicates(subset=['user_id', 'date_time','domain'], inplace=True)\n",
    "    df_domains = df['domain'].value_counts().reset_index()\n",
    "    # rename columns\n",
    "    df_domains.columns = ['domain', 'count']\n",
    "    # create csv file from daily domain count dataframe\n",
    "    df_domains.to_csv('daily_domains.csv',index=False, encoding='utf-8')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: \n",
    "Run the code in the following box. This will run for up to an hour. \n",
    "\n",
    "Wait until the circle in the top right corner is no longer full before moving on to next step.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 19516: expected 11 fields, saw 13\\n'\n",
      "b'Skipping line 102540: expected 11 fields, saw 16\\n'\n"
     ]
    }
   ],
   "source": [
    "# main code to process analyse daily log files and create domain count csv file\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "# loop through files in current directory which begin with 'ezproxy.log.*'\n",
    "# modify this if your log file name starts with something else\n",
    "# JC: I have modified this so that it will look for the log files in a particular folder \"/off/2012\"\n",
    "# JC: This also means that I do not have to delete the csv files later. However, I believe I do need to create separate folders under the \"csv\" folder \n",
    "\n",
    "\n",
    "for filename in glob.iglob(os.path.join(\"offcampus/201712/\",\"*.log\")):\n",
    "    filename = filename.replace(\"\\\\\",\"/\")\n",
    "    # call log analysis function \n",
    "    domain_count(filename)\n",
    "    # create dataframe from csv output file from daily domain count\n",
    "    df_daily = pd.read_csv('daily_domains.csv',sep=',', encoding='utf-8')\n",
    "    import os.path\n",
    "    # if domains.csv already exists - add daily domain counts to domains.csv\n",
    "    if os.path.isfile('domains.csv'):\n",
    "        # create dataframe from domains.csv\n",
    "        df = pd.read_csv('domains.csv',sep=',')\n",
    "        # rename daily dataframe count column to 'daily_count'\n",
    "        df_daily.rename(columns={'count': 'daily_count'}, inplace=True)\n",
    "        # merge the daily and accumulative dataframes\n",
    "        df_merge = pd.merge(df, df_daily, on='domain', how='outer')\n",
    "        # replace all NaN values with 0\n",
    "        df_merge.fillna(0, inplace=True)\n",
    "        # add new column with total of the two count columns\n",
    "        df_merge['total_count'] = df_merge['count'] + df_merge['daily_count']\n",
    "        # drop first 2 columns\n",
    "        df_merge.drop(df_merge.columns[[1,2]], axis=1, inplace=True)\n",
    "        # rename count column\n",
    "        df_merge.rename(columns={'total_count': 'count'}, inplace=True)\n",
    "        # create csv file from resulting dataframe\n",
    "        df_merge.to_csv('domains.csv',index=False, encoding='utf-8')\n",
    "    # first time through, convert domain count dataframe results to csv file\n",
    "    else:\n",
    "        df_daily.to_csv('domains.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4\n",
    "\n",
    "Check that the domains.csv file looks ok. It should have 2 columns: domain and count.\n",
    "\n",
    "Rename it to something like this: '2015_07_ezproxy_database_usage.csv'\n",
    "Delete daily_domains.csv\n",
    "Delete all of the log files that you have just analysed and copy and paste in the next month's worth of log\n",
    "files to be analysed.\n",
    "\n",
    "## Step 5:\n",
    "Repeat steps 3 and 4 until you have analysed all of the months that you want to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6:\n",
    "\n",
    "For each monthly file you created in the above process, you need to run the code in the next box. \n",
    "Each time you run it, you will need to modify the filename to the file that you are running.\n",
    "So this code starts with June 2016 - '2016_06_ezproxy_database_usage.csv'. The first time you run the code,\n",
    "you also need to change the column name in the 'else' section to the month name, in this case it's 'Jun-16'. \n",
    "For subsequent times you run it, change the month column name in the 'if' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combines monthly files into one spreadsheet\n",
    "import csv\n",
    "import pandas as pd\n",
    "# create dataframe from monthly domain count csv file\n",
    "# each time you run this code, change the file name as you work through each month\n",
    "# creates dataframe from monthly totals csv file\n",
    "df_one = pd.read_csv('2014_06_ezproxy_database_usage.csv',sep=',', encoding='utf-8')\n",
    "import os.path\n",
    "# second or more run through\n",
    "# merge monthly file with totals file\n",
    "if os.path.isfile('2013-2014_ezproxy_usage_monthly_totals.csv'):\n",
    "    # create dataframe from existing totals csv file\n",
    "    df_two = pd.read_csv('2013-2014_ezproxy_usage_monthly_totals.csv',sep=',', encoding='utf-8')\n",
    "    #drop rows with domain = 0\n",
    "    df_two = df_two[df_two.domain != \"0\"]\n",
    "    # merge two dataframes\n",
    "    df_merge = pd.merge(df_one, df_two, on='domain', how='outer')\n",
    "    # remove NaN values with 0\n",
    "    df_merge.fillna(0, inplace=True)\n",
    "    # rename count column to month/year\n",
    "    # change 'Jul-15' to whatever month file you are running\n",
    "    df_merge.rename(columns={'count': 'Jun-14'}, inplace=True)\n",
    "    # create csv from resulting dataframe\n",
    "    df_merge.to_csv('2013-2014_ezproxy_usage_monthly_totals.csv',index=False, encoding='utf-8')\n",
    "# first run through\n",
    "else:\n",
    "    # rename count of first monthly count column, so replace 'Jun-16' with the month you are starting with\n",
    "    df_one.rename(columns={'count': 'Jun-14'}, inplace=True)\n",
    "    #drop rows with domain = 0\n",
    "    df_one = df_one[df_one.domain != \"0\"]\n",
    "    # convert dataframe to csv\n",
    "    df_one.to_csv('2013-2014_ezproxy_usage_monthly_totals.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: \n",
    "\n",
    "Depending on how many months and which months you are processing, rename columns in the code below.\n",
    "Run the code below and your csv spreadsheet is complete. You can then open it in Excel, make it prettier and\n",
    "save it as a .xlsx file if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 6-month and yearly total columns\n",
    "import csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('2013-2014_ezproxy_usage_monthly_totals.csv',sep=',', encoding='utf-8')\n",
    "# Modified (from biannual total to total per semester)\n",
    "df['Sep-Dec_13_Total'] = df['Sep-13'] + df['Oct-13'] + df['Nov-13'] + df['Dec-13']\n",
    "df['Jan-Apr_14_Total'] = df['Jan-14'] + df['Feb-14'] + df['Mar-14'] + df['Apr-14'] \n",
    "df['May-Aug_14_Total'] = df['May-14'] + df['Jun-14'] + df['Jul-14'] + df['Aug-14']\n",
    "#df['Yearly_Total'] = df['Jul-Dec_13_Total'] + df['Jan-Jun_16_Total']\n",
    "df['Yearly_Total'] = df['Sep-Dec_13_Total'] + df['Jan-Apr_14_Total'] + df['May-Aug_14_Total']\n",
    "#sort on Yearly_Total\n",
    "df.sort_values(by='Yearly_Total', ascending=0, inplace=True)\n",
    "# create csv file from dataframe\n",
    "df.to_csv('2013-2014_ezproxy_usage_monthly_totals.csv',index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
